{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17542421",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e229062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Taxi-v3 ==\n",
      "# of actions: 6\n",
      "# of states: 500\n",
      "[[b'+' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'+']\n",
      " [b'|' b'R' b':' b' ' b'|' b' ' b':' b' ' b':' b'G' b'|']\n",
      " [b'|' b' ' b':' b' ' b'|' b' ' b':' b' ' b':' b' ' b'|']\n",
      " [b'|' b' ' b':' b' ' b':' b' ' b':' b' ' b':' b' ' b'|']\n",
      " [b'|' b' ' b'|' b' ' b':' b' ' b'|' b' ' b':' b' ' b'|']\n",
      " [b'|' b'Y' b'|' b' ' b':' b' ' b'|' b'B' b':' b' ' b'|']\n",
      " [b'+' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'-' b'+']]\n",
      "['P' 'P' 'P' 'P' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'D' 'S'\n",
      " 'S' 'S' 'W' 'W' 'W' 'W' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S'\n",
      " 'W' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'E' 'E' 'E' 'E' 'S' 'S' 'S' 'S' 'S' 'S'\n",
      " 'S' 'S' 'S' 'E' 'S' 'S' 'S' 'S' 'S' 'S' 'E' 'E' 'E' 'E' 'S' 'S' 'S' 'S'\n",
      " 'S' 'S' 'S' 'S' 'S' 'E' 'S' 'S' 'S' 'S' 'S' 'S' 'P' 'P' 'P' 'P' 'S' 'S'\n",
      " 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'D' 'S' 'S' 'N' 'N' 'N' 'N' 'S' 'S' 'S' 'S'\n",
      " 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'N' 'S' 'S' 'S' 'N' 'N' 'N' 'N' 'S' 'S'\n",
      " 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'N' 'S' 'S' 'S' 'S' 'S' 'S' 'S'\n",
      " 'N' 'N' 'N' 'N' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'N' 'S' 'S' 'S' 'S'\n",
      " 'S' 'S' 'N' 'N' 'N' 'N' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'N' 'S' 'S'\n",
      " 'S' 'S' 'S' 'S' 'N' 'N' 'N' 'N' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'N'\n",
      " 'S' 'S' 'N' 'N' 'N' 'N' 'E' 'E' 'E' 'E' 'S' 'S' 'S' 'S' 'E' 'E' 'E' 'E'\n",
      " 'N' 'E' 'S' 'E' 'N' 'N' 'N' 'N' 'E' 'E' 'E' 'E' 'W' 'W' 'W' 'W' 'E' 'E'\n",
      " 'E' 'E' 'N' 'E' 'W' 'E' 'W' 'W' 'W' 'W' 'N' 'N' 'N' 'N' 'W' 'W' 'W' 'W'\n",
      " 'E' 'E' 'E' 'E' 'W' 'N' 'W' 'E' 'W' 'W' 'W' 'W' 'N' 'N' 'N' 'N' 'W' 'W'\n",
      " 'W' 'W' 'S' 'S' 'S' 'S' 'W' 'N' 'W' 'S' 'W' 'W' 'W' 'W' 'N' 'N' 'N' 'N'\n",
      " 'W' 'W' 'W' 'W' 'S' 'S' 'S' 'S' 'W' 'N' 'W' 'S' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'S' 'S' 'S' 'S' 'N' 'N' 'N' 'N' 'N' 'N' 'S' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'S' 'S' 'S' 'S' 'N' 'N'\n",
      " 'N' 'S' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'S' 'S' 'S' 'S'\n",
      " 'N' 'N' 'N' 'S' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'P' 'P' 'P' 'P' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'D' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'P' 'P' 'P' 'P' 'N' 'N' 'N' 'D' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'N' 'N' 'W' 'W' 'W' 'W' 'N' 'N' 'N' 'W']\n",
      "['P' 'P' 'P' 'P' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'D' 'S'\n",
      " 'S' 'S' 'W' 'W' 'W' 'W' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S'\n",
      " 'W' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'E' 'E' 'E' 'E' 'S' 'S' 'S' 'S' 'S' 'S'\n",
      " 'S' 'S' 'S' 'E' 'S' 'S' 'S' 'S' 'S' 'S' 'E' 'E' 'E' 'E' 'S' 'S' 'S' 'S'\n",
      " 'S' 'S' 'S' 'S' 'S' 'E' 'S' 'S' 'S' 'S' 'S' 'S' 'P' 'P' 'P' 'P' 'S' 'S'\n",
      " 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'D' 'S' 'S' 'N' 'N' 'N' 'N' 'S' 'S' 'S' 'S'\n",
      " 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'N' 'S' 'S' 'S' 'N' 'N' 'N' 'N' 'S' 'S'\n",
      " 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'N' 'S' 'S' 'S' 'S' 'S' 'S' 'S'\n",
      " 'N' 'N' 'N' 'N' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'N' 'S' 'S' 'S' 'S'\n",
      " 'S' 'S' 'N' 'N' 'N' 'N' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'N' 'S' 'S'\n",
      " 'S' 'S' 'S' 'S' 'N' 'N' 'N' 'N' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'N'\n",
      " 'S' 'S' 'N' 'N' 'N' 'N' 'E' 'E' 'E' 'E' 'S' 'S' 'S' 'S' 'E' 'E' 'E' 'E'\n",
      " 'N' 'E' 'S' 'E' 'N' 'N' 'N' 'N' 'E' 'E' 'E' 'E' 'W' 'W' 'W' 'W' 'E' 'E'\n",
      " 'E' 'E' 'N' 'E' 'W' 'E' 'W' 'W' 'W' 'W' 'N' 'N' 'N' 'N' 'W' 'W' 'W' 'W'\n",
      " 'E' 'E' 'E' 'E' 'W' 'N' 'W' 'E' 'W' 'W' 'W' 'W' 'N' 'N' 'N' 'N' 'W' 'W'\n",
      " 'W' 'W' 'S' 'S' 'S' 'S' 'W' 'N' 'W' 'S' 'W' 'W' 'W' 'W' 'N' 'N' 'N' 'N'\n",
      " 'W' 'W' 'W' 'W' 'S' 'S' 'S' 'S' 'W' 'N' 'W' 'S' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'S' 'S' 'S' 'S' 'N' 'N' 'N' 'N' 'N' 'N' 'S' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'S' 'S' 'S' 'S' 'N' 'N'\n",
      " 'N' 'S' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'S' 'S' 'S' 'S'\n",
      " 'N' 'N' 'N' 'S' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'P' 'P' 'P' 'P' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'D' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'P' 'P' 'P' 'P' 'N' 'N' 'N' 'D' 'N' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'N' 'N' 'W' 'W' 'W' 'W' 'N' 'N' 'N' 'W']\n",
      "Discrepancy: 0\n"
     ]
    }
   ],
   "source": [
    "def get_rewards_and_transitions_from_env(env):\n",
    "    # Get state and action space sizes\n",
    "    num_states = env.observation_space.n\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    # Intiailize matrices\n",
    "    R = np.zeros((num_states, num_actions, num_states))\n",
    "    P = np.zeros((num_states, num_actions, num_states))\n",
    "\n",
    "    # Get rewards and transition probabilitites for all transitions from an OpenAI gym environment\n",
    "    for s in range(num_states):\n",
    "        for a in range(num_actions):\n",
    "            for transition in env.P[s][a]:\n",
    "                prob, s_, r, done = transition\n",
    "                R[s, a, s_] = r\n",
    "                P[s, a, s_] = prob\n",
    "                \n",
    "    return R, P\n",
    "\n",
    "def value_iteration(env, gamma=0.9, max_iterations=10**6, eps=10**-3):\n",
    "    \"\"\"        \n",
    "        Run value iteration (You probably need no more than 30 lines)\n",
    "        \n",
    "        Input Arguments\n",
    "        ----------\n",
    "            env: \n",
    "                the target environment\n",
    "            gamma: float\n",
    "                the discount factor for rewards\n",
    "            max_iterations: int\n",
    "                maximum number of iterations for value iteration\n",
    "            eps: float\n",
    "                for the termination criterion of value iteration \n",
    "        ----------\n",
    "        \n",
    "        Output\n",
    "        ----------\n",
    "            policy: np.array of size (500,)\n",
    "        ----------\n",
    "        \n",
    "        TODOs\n",
    "        ----------\n",
    "            1. Initialize the value function V(s)\n",
    "            2. Get transition probabilities and reward function from the gym env\n",
    "            3. Iterate and improve V(s) using the Bellman optimality operator\n",
    "            4. Derive the optimal policy using V(s)\n",
    "        ----------\n",
    "    \"\"\"\n",
    "    num_spaces = env.observation_space.n\n",
    "    num_actions = env.action_space.n\n",
    "    \n",
    "    # Initialize with a random policy\n",
    "    policy = np.array([env.action_space.sample() for _ in range(num_spaces)])\n",
    "    \n",
    "    ##### FINISH TODOS HERE #####\n",
    "    V=np.zeros(num_spaces)\n",
    "    R, P=get_rewards_and_transitions_from_env(env)\n",
    "    for i in range(max_iterations):\n",
    "        V_next=V.copy()\n",
    "        delta=0\n",
    "        for s in range(num_spaces):\n",
    "            M_a=0\n",
    "            M_v=0\n",
    "            for a in range(num_actions):\n",
    "                v=0\n",
    "                for s_ in range(num_spaces):\n",
    "                    v +=R[s][a][s_]+gamma*P[s][a][s_]*V_next[s_]\n",
    "                    if v>M_v:\n",
    "                        M_v=v\n",
    "                        M_a=a\n",
    "            V[s]=M_v\n",
    "            policy[s]=M_a\n",
    "            delta=max(delta,abs(V[s] - V_next[s]))\n",
    "        if delta<eps:\n",
    "            break\n",
    "    #############################\n",
    "    \n",
    "    # Return optimal policy    \n",
    "    return policy\n",
    "\n",
    "def policy_iteration(env, gamma=0.9, max_iterations=10**6, eps=10**-3):\n",
    "    \"\"\" \n",
    "        Run policy iteration (You probably need no more than 30 lines)\n",
    "        \n",
    "        Input Arguments\n",
    "        ----------\n",
    "            env: \n",
    "                the target environment\n",
    "            gamma: float\n",
    "                the discount factor for rewards\n",
    "            max_iterations: int\n",
    "                maximum number of iterations for the policy evalaution in policy iteration\n",
    "            eps: float\n",
    "                for the termination criterion of policy evaluation \n",
    "        ----------  \n",
    "        \n",
    "        Output\n",
    "        ----------\n",
    "            policy: np.array of size (500,)\n",
    "        ----------\n",
    "        \n",
    "        TODOs\n",
    "        ----------\n",
    "            1. Initialize with a random policy and initial value function\n",
    "            2. Get transition probabilities and reward function from the gym env\n",
    "            3. Iterate and improve the policy\n",
    "        ----------\n",
    "    \"\"\"\n",
    "    num_spaces = env.observation_space.n\n",
    "    num_actions = env.action_space.n\n",
    "    \n",
    "    # Initialize with a random policy\n",
    "    policy = np.array([env.action_space.sample() for _ in range(num_spaces)])\n",
    "    \n",
    "    ##### FINISH TODOS HERE #####\n",
    "    V=np.zeros(num_spaces)\n",
    "    R, P = get_rewards_and_transitions_from_env(env)\n",
    "    flag = 1\n",
    "    p_next=policy.copy()\n",
    "    while  (policy - p_next).any()!=0 or flag==1:\n",
    "        p_next= policy.copy()\n",
    "        if flag:\n",
    "            flag=0\n",
    "        for i in range(max_iterations):\n",
    "            V_next=V.copy()\n",
    "            delta=0\n",
    "            for s in range(num_spaces):\n",
    "                v=0\n",
    "                for s_ in range(num_spaces):\n",
    "                    v+=R[s][policy[s]][s_]+gamma*P[s][policy[s]][s_]*V_next[s_]\n",
    "                V[s]=v\n",
    "                delta=max(delta, abs(V[s] - V_next[s]))\n",
    "            if delta<eps:\n",
    "                break\n",
    "        for s in range(num_spaces):\n",
    "            M_v=0\n",
    "            M_a=0\n",
    "            for a in range(num_actions):\n",
    "                v=0\n",
    "                for s_ in range(num_spaces):\n",
    "                    v+=R[s][a][s_]+gamma*P[s][a][s_]*V_next[s_]\n",
    "                if v>M_v:\n",
    "                    M_v=v\n",
    "                    M_a=a\n",
    "            #V[s]=M_v\n",
    "            policy[s]=M_a\n",
    "    \n",
    "    #############################\n",
    "\n",
    "    # Return optimal policy\n",
    "    return policy\n",
    "\n",
    "def print_policy(policy, mapping=None, shape=(0,)):\n",
    "    print(np.array([mapping[action] for action in policy]).reshape(shape))\n",
    "\n",
    "\n",
    "def run_pi_and_vi(env_name):\n",
    "    \"\"\" \n",
    "        Enforce policy iteration and value iteration\n",
    "    \"\"\"    \n",
    "    env = gym.make(env_name)\n",
    "    print('== {} =='.format(env_name))\n",
    "    print('# of actions:', env.action_space.n)\n",
    "    print('# of states:', env.observation_space.n)\n",
    "    print(env.desc)\n",
    "\n",
    "    vi_policy = value_iteration(env)\n",
    "    pi_policy = policy_iteration(env)\n",
    "\n",
    "    return pi_policy, vi_policy\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # OpenAI gym environment: Taxi-v2 or Taxi-v3\n",
    "    pi_policy, vi_policy = run_pi_and_vi('Taxi-v3')\n",
    "\n",
    "    # For debugging\n",
    "    action_map = {0: \"S\", 1: \"N\", 2: \"E\", 3: \"W\", 4: \"P\", 5: \"D\"}\n",
    "    print_policy(pi_policy, action_map, shape=None)\n",
    "    print_policy(vi_policy, action_map, shape=None)\n",
    "    \n",
    "    # Compare the policies obatined via policy iteration and value iteration\n",
    "    diff = sum([abs(x-y) for x, y in zip(pi_policy.flatten(), vi_policy.flatten())])        \n",
    "    print('Discrepancy:', diff)\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2417e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
